{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8CoiNMU1IQ8"
   },
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VNPgCs7b1Jso"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za0s4Egl1QU5"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RlMVRmdS1R0l"
   },
   "outputs": [],
   "source": [
    "# Function to remove missing values above a given threshold\n",
    "def missing_remove(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isna().sum()/dataframe.shape[1])>threshold].tolist()\n",
    "    print(f\"# Features deleted with more than {threshold} % missing values\", len(columns))\n",
    "    return dataframe.drop(columns, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i62Cd7qc1LeI"
   },
   "outputs": [],
   "source": [
    "# Function to remove features with unique values\n",
    "def unique_remove(dataframe):\n",
    "    unique_features = []\n",
    "    for col in dataframe.columns:\n",
    "        if(dataframe[col].unique().size == 2):\n",
    "            if(len(unique_features) == 0):\n",
    "                unique_features =  dataframe[col]\n",
    "            else:\n",
    "                 unique_features = pd.concat([unique_features, dataframe[col]], axis=1)\n",
    "            dataframe.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j_OUWkYC1WQD"
   },
   "outputs": [],
   "source": [
    "# Function to remove outlier\n",
    "def outlier_detection_zcore(dataframe):\n",
    "    data_mean, data_std  = dataframe.mean(), dataframe.std()\n",
    "    data_z_scores = ((dataframe - data_mean) / data_std).abs()\n",
    "    m = data_z_scores > 3\n",
    "    dataframe = dataframe.mask(m,inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYimj7lt1efy"
   },
   "source": [
    "## Importing and basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LbJIxqWC1hBn"
   },
   "outputs": [],
   "source": [
    "# Importing data from internet in runtime\n",
    "data_url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vLONGBRz1iz7"
   },
   "outputs": [],
   "source": [
    "# Assigning data to a pandas dataframe\n",
    "secom_data = pd.read_csv(data_url,sep=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "mJ-CpITl1lJG",
    "outputId": "b3a93674-6003-41b9-c2b3-ad8f5c16698c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_581</th>\n",
       "      <th>Feature_582</th>\n",
       "      <th>Feature_583</th>\n",
       "      <th>Feature_584</th>\n",
       "      <th>Feature_585</th>\n",
       "      <th>Feature_586</th>\n",
       "      <th>Feature_587</th>\n",
       "      <th>Feature_588</th>\n",
       "      <th>Feature_589</th>\n",
       "      <th>Feature_590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    3030.93    2564.00  2187.7333  1411.1265     1.3602      100.0   \n",
       "1    3095.78    2465.14  2230.4222  1463.6606     0.8294      100.0   \n",
       "2    2932.61    2559.94  2186.4111  1698.0172     1.5102      100.0   \n",
       "3    2988.72    2479.90  2199.0333   909.7926     1.3204      100.0   \n",
       "4    3032.24    2502.87  2233.3667  1326.5200     1.5334      100.0   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_581  Feature_582  \\\n",
       "0    97.6133     0.1242     1.5005      0.0162  ...          NaN          NaN   \n",
       "1   102.3433     0.1247     1.4966     -0.0005  ...       0.0060     208.2045   \n",
       "2    95.4878     0.1241     1.4436      0.0041  ...       0.0148      82.8602   \n",
       "3   104.2367     0.1217     1.4882     -0.0124  ...       0.0044      73.8432   \n",
       "4   100.3967     0.1235     1.5031     -0.0031  ...          NaN          NaN   \n",
       "\n",
       "   Feature_583  Feature_584  Feature_585  Feature_586  Feature_587  \\\n",
       "0       0.5005       0.0118       0.0035       2.3630          NaN   \n",
       "1       0.5019       0.0223       0.0055       4.4447       0.0096   \n",
       "2       0.4958       0.0157       0.0039       3.1745       0.0584   \n",
       "3       0.4990       0.0103       0.0025       2.0544       0.0202   \n",
       "4       0.4800       0.4766       0.1045      99.3032       0.0202   \n",
       "\n",
       "   Feature_588  Feature_589  Feature_590  \n",
       "0          NaN          NaN          NaN  \n",
       "1       0.0201       0.0060     208.2045  \n",
       "2       0.0484       0.0148      82.8602  \n",
       "3       0.0149       0.0044      73.8432  \n",
       "4       0.0149       0.0044      73.8432  \n",
       "\n",
       "[5 rows x 590 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming the columns \n",
    "secom_data.columns = [\"Feature_\"+str(column+1) for column in range(len(secom_data.columns))]\n",
    "secom_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1m82GHOT1msU"
   },
   "outputs": [],
   "source": [
    "# Importing target data from internet in runtime\n",
    "label_url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MXNuzPJe1sNk"
   },
   "outputs": [],
   "source": [
    "# Assigning target data to a pandas dataframe\n",
    "secom_labels = pd.read_csv(label_url, sep = \" \",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RqtL5guL1tz7"
   },
   "outputs": [],
   "source": [
    "# Renaming the columns \n",
    "secom_labels.columns = [\"Classification\",\"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "84Dl-Fay1vz3"
   },
   "outputs": [],
   "source": [
    "# Changing datatype for consitency\n",
    "secom_labels['Timestamp'] = pd.to_datetime(secom_labels['Timestamp'],errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qSmgspyk1xcL"
   },
   "outputs": [],
   "source": [
    "# Merging the data\n",
    "data= pd.concat([secom_labels,secom_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "foQ0Zplo1yxq"
   },
   "outputs": [],
   "source": [
    "# Dropping the timestamp column as it provides no insights\n",
    "data.drop(columns=\"Timestamp\", errors='raise',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wSd7j_0210WF"
   },
   "outputs": [],
   "source": [
    "target = data[['Classification']]\n",
    "data.drop(['Classification'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcnUqd02122m"
   },
   "source": [
    "## Spliting the data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UfkKHvKQ14hm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,target, test_size=0.25, random_state=42, stratify=target)\n",
    "# We use the startify parameter as the data is imbalanced on the basis of pass and fail and this will ensure the same \n",
    "#is retained when we split - Random state will help us create a reproducible data - Test size is the split ration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VngQ8qmq18L2"
   },
   "source": [
    "## Re-labeling the Target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Zita67WD18oW"
   },
   "outputs": [],
   "source": [
    "# Relabeling target variables 1 is pass and 0 is fail\n",
    "y_train = y_train.replace(to_replace=[-1, 1], value=[1, 0])\n",
    "y_test = y_test.replace(to_replace=[-1, 1], value=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Qqf5h04o2BV3"
   },
   "outputs": [],
   "source": [
    "# # Scaling the data\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "# X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ktw4-If-2JVo"
   },
   "source": [
    "## Removing columns with more that 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sealjvqy16tt",
    "outputId": "164500ed-df5c-4369-d435-962dbc837897"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1175, 590)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3sSkLKz2MQv",
    "outputId": "cdb55c6a-73da-4471-bd1c-24115ab55427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Features deleted with more than 0.5 % missing values 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1175, 558)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_remove(X_train, 0.5)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ch_hcYMU2OJQ"
   },
   "outputs": [],
   "source": [
    "## Removing Columns with non unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nKubTxv2Qio",
    "outputId": "6087e77d-932c-40a9-fa07-26da8eb42ad1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1175, 442)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_remove(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODaHzF7B2SS0",
    "outputId": "4ad0e77b-18b1-4436-927c-bb2000ec5948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 442)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From test set deleting columns that are removed in training set\n",
    "X_test = X_test[np.array(X_train.columns)]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def Outlier(X) :\n",
    "       \n",
    "        z_train= pd.DataFrame(stats.zscore(X,nan_policy='omit'))\n",
    "        z_train\n",
    "       \n",
    "        print('Number of Outliers : ',sum(z_train.apply(lambda x: sum(x.apply(lambda x: 1 if abs(x)>3 else 0)))))\n",
    "\n",
    "        is_outlier = abs(z_train)>3\n",
    "        is_outlier.columns = X.columns\n",
    "        is_outlier.index = X.index\n",
    "        return(X.mask(is_outlier, np.nan))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_test.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "X_train = Outlier(X_train)\n",
    "X_test = Outlier(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute na values using MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "MICE_imputer = IterativeImputer(random_state=100)\n",
    "X_train_MICE = X_train.copy(deep=True)\n",
    "X_train_MICE.iloc[:,:] = pd.DataFrame(MICE_imputer.fit_transform(X_train))\n",
    "X_train = X_train_MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfscore = RandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "clfscore.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print ('RF accuracy: TRAINING', clfscore.score(X_train,y_train.values.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boruta function\n",
    "\n",
    "def BorutaFeatureSelection (X, y) :\n",
    "    feature_names = np.array(X.columns)\n",
    "\n",
    "    model = model = RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample', max_depth=5, random_state=100)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    feature_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=100, max_iter=100)\n",
    "\n",
    "    feature_selector.fit(X.to_numpy(),y)\n",
    "\n",
    "    feature_selector.support_\n",
    "\n",
    "    feature_selector.ranking_\n",
    "\n",
    "    feature_ranks = list(zip(feature_names, \n",
    "                             feature_selector.ranking_, \n",
    "                             feature_selector.support_))\n",
    "\n",
    "    for feat in feature_ranks:\n",
    "        print('Feature: {:<30} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "        \n",
    "    final_features = list()\n",
    "    indexes = np.where(feature_selector.ranking_ <= 1)\n",
    "    for x in np.nditer(indexes):\n",
    "        final_features.append(feature_names[x])\n",
    "    print(final_features)\n",
    "    \n",
    "    return pd.DataFrame(X.filter(final_features)) , final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "X_train , final_features = BorutaFeatureSelection(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_test = Outlier(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_MICE = X_test.copy(deep=True)\n",
    "X_test_MICE.iloc[:,:] = pd.DataFrame(MICE_imputer.fit_transform(X_test))\n",
    "X_test = X_test_MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.filter(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfscore = RandomForestClassifier(n_estimators=100, max_depth=2)\n",
    "clfscore.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print ('RF accuracy: TRAINING', clfscore.score(X_train,y_train))\n",
    "print ('RF accuracy: TESTING', clfscore.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from scikitplot.metrics import plot_roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", yticklabels = ['Pred. Pass' , 'Pred. Fail'] , \n",
    "            xticklabels = ['Act. Pass', 'Act. Fail'])\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test Accuracy Score\", accuracy_score(y_test, y_pred))\n",
    "metrics.RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BALANCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "g = pca.transform(X_train)\n",
    "g = pd.DataFrame(g, columns=['a','b'])\n",
    "g['Classification'] = y_train\n",
    "sns.scatterplot(data = g , x='a' , y='b', hue = g['Classification'],legend= True )\n",
    "print(pca.explained_variance_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling technique\n",
    "\n",
    "def Sampling(X_train, y_train, sampler):\n",
    "\n",
    "    #SMOTE\n",
    "    if sampler == 'SMOTE':\n",
    "        sampler = SMOTE(random_state=100)    \n",
    "    \n",
    "    #ROSE\n",
    "    if sampler == 'ROSE':\n",
    "        sampler = RandomOverSampler(random_state=100, shrinkage=1)\n",
    "\n",
    "    #ADASYN\n",
    "    if sampler == 'ADASYN':\n",
    "        sampler = ADASYN(random_state=100)\n",
    "    \n",
    "\n",
    "    #SMOTTEENN\n",
    "    if sampler == 'SMOTEENN' :\n",
    "        sampler = SMOTEENN(random_state=100)\n",
    "        \n",
    "        \n",
    "    #Random under Sampling\n",
    "    if sampler == \"randomunder\":\n",
    "        sampler = RandomUnderSampler(random_state=100)\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    counter = Counter(y_resampled)\n",
    "    print(counter)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance treatment\n",
    "from collections import Counter\n",
    "X_resampled, y_resampled = Sampling(X_train, y_train,'SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_resampled)\n",
    "g = pca.transform(X_resampled)\n",
    "g = pd.DataFrame(g, columns=['a','b'])\n",
    "g['Classification'] = y_resampled\n",
    "sns.scatterplot(data = g , x='a' , y='b', hue= g['Classification'], legend= True )\n",
    "print(pca.explained_variance_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance treatment\n",
    "X_resampled1, y_resampled1 = Sampling(X_train, y_train,'ADASYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_resampled1)\n",
    "g = pca.transform(X_resampled1)\n",
    "g = pd.DataFrame(g, columns=['a','b'])\n",
    "g['Classification'] = y_resampled1\n",
    "sns.scatterplot(data = g , x='a' , y='b', hue= g['Classification'], legend= True )\n",
    "print(pca.explained_variance_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance treatment\n",
    "X_resampled2, y_resampled2 = Sampling(X_train, y_train,'ROSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_resampled2)\n",
    "g = pca.transform(X_resampled2)\n",
    "g = pd.DataFrame(g, columns=['a','b'])\n",
    "g['Classification'] = y_resampled2\n",
    "sns.scatterplot(data = g , x='a' , y='b', hue= g['Classification'], legend= True )\n",
    "print(pca.explained_variance_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance treatment\n",
    "from imblearn.combine import SMOTEENN\n",
    "X_resampled3, y_resampled3 = Sampling(X_train, y_train,'SMOTEENN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_resampled3)\n",
    "g = pca.transform(X_resampled3)\n",
    "g = pd.DataFrame(g, columns=['a','b'])\n",
    "g['Classification'] = y_resampled3\n",
    "sns.scatterplot(data = g , x='a' , y='b', hue= g['Classification'], legend= True )\n",
    "print(pca.explained_variance_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance treatment\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_resampled4, y_resampled4 = Sampling(X_train, y_train,'randomunder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_resampled4)\n",
    "g = pca.transform(X_resampled4)\n",
    "g = pd.DataFrame(g, columns=['a','b'])\n",
    "g['Classification'] = y_resampled4\n",
    "sns.scatterplot(data = g , x='a' , y='b', hue= g['Classification'], legend= True )\n",
    "print(pca.explained_variance_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "#Percentage of variance explained by each of the selected components"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "base file.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
