{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8CoiNMU1IQ8"
   },
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VNPgCs7b1Jso"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import accuracy_score,recall_score, precision_score\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn(\"this will not show\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za0s4Egl1QU5"
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RlMVRmdS1R0l"
   },
   "outputs": [],
   "source": [
    "# Function to remove missing values above a given threshold\n",
    "def missing_remove(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isna().sum()/dataframe.shape[1])>threshold].tolist()\n",
    "    print(f\"# Features deleted with more than {threshold} % missing values\", len(columns))\n",
    "    return dataframe.drop(columns, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i62Cd7qc1LeI"
   },
   "outputs": [],
   "source": [
    "# Function to remove features with unique values\n",
    "def unique_remove(dataframe):\n",
    "    unique_features = []\n",
    "    for col in dataframe.columns:\n",
    "        if(dataframe[col].unique().size == 2):\n",
    "            if(len(unique_features) == 0):\n",
    "                unique_features =  dataframe[col]\n",
    "            else:\n",
    "                 unique_features = pd.concat([unique_features, dataframe[col]], axis=1)\n",
    "            dataframe.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j_OUWkYC1WQD"
   },
   "outputs": [],
   "source": [
    "# Function to remove outlier\n",
    "def outlier_detection_zcore(dataframe):\n",
    "    data_mean, data_std  = dataframe.mean(), dataframe.std()\n",
    "    data_z_scores = ((dataframe - data_mean) / data_std).abs()\n",
    "    m = data_z_scores > 3\n",
    "    dataframe = dataframe.mask(m,inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ICLy3MkK4OTH"
   },
   "outputs": [],
   "source": [
    "def rfc(X_train_rfc,X_test_rfc,y_train,max_depth):\n",
    "  from matplotlib.pyplot import figure\n",
    "  figure(figsize=(8, 5), dpi=80)\n",
    "  model = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=max_depth)\n",
    "  model.fit(X_train_rfc, y_train)\n",
    "  y_pred = model.predict(X_test_rfc)\n",
    "  sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\",cmap=\"YlOrRd\",yticklabels = ['Act. Pass', 'Act. Fail'], xticklabels = ['Pred. Pass' , 'Pred. Fail'])\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  ax = plt.gca()\n",
    "  plot = plot_roc_curve(model, X_test_rfc, y_test, ax=ax, alpha=0.8)\n",
    "  print(\"Test Accuracy Score\", accuracy_score(y_test, y_pred))\n",
    "  f1score = f1_score(y_test, y_pred, average='micro')\n",
    "  mccscore=matthews_corrcoef(y_test, y_pred)\n",
    "  return f1score, mccscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6c2XYU0bnD9D"
   },
   "outputs": [],
   "source": [
    "def evaluate1(model, X_train,X_test,y_train,y_test):\n",
    "  from matplotlib.pyplot import figure\n",
    "  figure(figsize=(8, 5), dpi=80)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(X_train)\n",
    "  train_std = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "  test_std = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  cm=confusion_matrix(y_test, y_pred)\n",
    "  #print(classification_report(y_test, y_pred))\n",
    "  ax = plt.gca()\n",
    "  plot = plot_roc_curve(model, X_test, y_test, ax=ax, alpha=0.8)\n",
    "  print(\"Train Accuracy Score:\", accuracy_score(y_train, model.predict(X_train)))\n",
    "  print(\"Test Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "  f1score = f1_score(y_test, y_pred, average='micro')\n",
    "  mccscore=matthews_corrcoef(y_test, y_pred)\n",
    "  recall = recall_score(y_test,y_pred)\n",
    "  precision = precision_score(y_test,y_pred)\n",
    "  specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "  sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "  FNR = cm[1,0]/(cm[1,0]+cm[0,0])\n",
    "  FPR = cm[0,1]/(cm[0,1]+cm[1,1])\n",
    "  specificity1 = 1 - FPR\n",
    "  sensitivity1 = 1- FNR\n",
    "  print(\"F1 Score:\", f1score) \n",
    "  print(\"MC Score:\" , mccscore)\n",
    "  print(\"Sensitivity:\" , sensitivity)\n",
    "  print(\"Specificity:\" , specificity)\n",
    "  print(\"Recall:\" , recall)\n",
    "  print(\"Precision:\" , precision)\n",
    "  print(\"FNR:\" , FNR)\n",
    "  print(\"FPR:\" , FPR)\n",
    "  #print(\"Sensitivity1:\" , sensitivity1)\n",
    "  #print(\"Specificity1:\" , specificity1)\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\",cmap=\"YlOrRd\",yticklabels = ['Act. Pass', 'Act. Fail'], xticklabels = ['Pred. Pass' , 'Pred. Fail'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VQdL9-YfZKNc"
   },
   "outputs": [],
   "source": [
    "def evaluate2(model, X_train,X_test,y_train,y_test):\n",
    "  from matplotlib.pyplot import figure\n",
    "  figure(figsize=(8, 5), dpi=80)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(X_train)\n",
    "  X_train_std = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "  X_test_std = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "  model.fit(X_train_std, y_train)\n",
    "  y_pred = model.predict(X_test_std)\n",
    "  cm=confusion_matrix(y_test, y_pred)\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\",cmap=\"YlOrRd\",yticklabels = ['Act. Pass', 'Act. Fail'], xticklabels = ['Pred. Pass' , 'Pred. Fail'])\n",
    "  #print(classification_report(y_test, y_pred))\n",
    "  ax = plt.gca()\n",
    "  plot = plot_roc_curve(model, X_test_std, y_test, ax=ax, alpha=0.8)\n",
    "  print(\"Model Name:\", model)\n",
    "  print(\"Train Accuracy Score:\", accuracy_score(y_train, model.predict(X_train_std)))\n",
    "  print(\"Test Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "  f1score = f1_score(y_test, y_pred, average='micro')\n",
    "  mccscore=matthews_corrcoef(y_test, y_pred)\n",
    "  recall = recall_score(y_test,y_pred)\n",
    "  precision = precision_score(y_test,y_pred)\n",
    "  specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "  sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "  FNR = cm[1,0]/(cm[1,0]+cm[0,0])\n",
    "  FPR = cm[0,1]/(cm[0,1]+cm[1,1])\n",
    "  specificity1 = 1 - FPR\n",
    "  sensitivity1 = 1- FNR\n",
    "  print(\"F1 Score:\", f1score) \n",
    "  print(\"MC Score:\" , mccscore)\n",
    "  print(\"Sensitivity:\" , sensitivity)\n",
    "  print(\"Specificity:\" , specificity)\n",
    "  print(\"Recall:\" , recall)\n",
    "  print(\"Precision:\" , precision)\n",
    "  print(\"FNR:\" , FNR)\n",
    "  print(f\"FPR: {FPR}\\n\" )\n",
    "  #print(\"Sensitivity1:\" , sensitivity1)\n",
    "  #print(\"Specificity1:\" , specificity1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CWlgaZXXafAh"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from yellowbrick.classifier import ConfusionMatrix, ClassPredictionError\n",
    "def evaluate(train_df, test_df, train_target, test_target):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df)\n",
    "    train_std = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns)\n",
    "    test_std = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n",
    "    \n",
    "    logreg = LogisticRegression(random_state = 42, class_weight='balanced', C=200, dual=False, solver='liblinear')\n",
    "    logreg.fit(train_std, train_target.values.ravel())\n",
    "    \n",
    "    y_pred = logreg.predict(test_std)\n",
    "    y_true = test_target.values.ravel()\n",
    "    f1score = f1_score(y_true, y_pred, average='micro')\n",
    "    mccscore=matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "   \n",
    "    cm = ConfusionMatrix(logreg, classes=['Fail', 'Pass'], cmap=\"YlGn\", size=(400,400))\n",
    "    cm.score(test_std, y_true)\n",
    "    cm.show()\n",
    "    from matplotlib.pyplot import figure\n",
    "    figure(figsize=(5, 5), dpi=80)\n",
    "    ax = plt.gca()\n",
    "    plot = plot_roc_curve(logreg, test_df, test_target, ax=ax, alpha=0.8)\n",
    "    \n",
    "    return f1score, mccscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYimj7lt1efy"
   },
   "source": [
    "#### Importing and basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LbJIxqWC1hBn"
   },
   "outputs": [],
   "source": [
    "# Importing data from internet in runtime\n",
    "data_url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vLONGBRz1iz7"
   },
   "outputs": [],
   "source": [
    "# Assigning data to a pandas dataframe\n",
    "secom_data = pd.read_csv(data_url,sep=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "mJ-CpITl1lJG",
    "outputId": "abfc9438-fba3-4088-9917-a83a2ef036fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_581</th>\n",
       "      <th>Feature_582</th>\n",
       "      <th>Feature_583</th>\n",
       "      <th>Feature_584</th>\n",
       "      <th>Feature_585</th>\n",
       "      <th>Feature_586</th>\n",
       "      <th>Feature_587</th>\n",
       "      <th>Feature_588</th>\n",
       "      <th>Feature_589</th>\n",
       "      <th>Feature_590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    3030.93    2564.00  2187.7333  1411.1265     1.3602      100.0   \n",
       "1    3095.78    2465.14  2230.4222  1463.6606     0.8294      100.0   \n",
       "2    2932.61    2559.94  2186.4111  1698.0172     1.5102      100.0   \n",
       "3    2988.72    2479.90  2199.0333   909.7926     1.3204      100.0   \n",
       "4    3032.24    2502.87  2233.3667  1326.5200     1.5334      100.0   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_581  Feature_582  \\\n",
       "0    97.6133     0.1242     1.5005      0.0162  ...          NaN          NaN   \n",
       "1   102.3433     0.1247     1.4966     -0.0005  ...       0.0060     208.2045   \n",
       "2    95.4878     0.1241     1.4436      0.0041  ...       0.0148      82.8602   \n",
       "3   104.2367     0.1217     1.4882     -0.0124  ...       0.0044      73.8432   \n",
       "4   100.3967     0.1235     1.5031     -0.0031  ...          NaN          NaN   \n",
       "\n",
       "   Feature_583  Feature_584  Feature_585  Feature_586  Feature_587  \\\n",
       "0       0.5005       0.0118       0.0035       2.3630          NaN   \n",
       "1       0.5019       0.0223       0.0055       4.4447       0.0096   \n",
       "2       0.4958       0.0157       0.0039       3.1745       0.0584   \n",
       "3       0.4990       0.0103       0.0025       2.0544       0.0202   \n",
       "4       0.4800       0.4766       0.1045      99.3032       0.0202   \n",
       "\n",
       "   Feature_588  Feature_589  Feature_590  \n",
       "0          NaN          NaN          NaN  \n",
       "1       0.0201       0.0060     208.2045  \n",
       "2       0.0484       0.0148      82.8602  \n",
       "3       0.0149       0.0044      73.8432  \n",
       "4       0.0149       0.0044      73.8432  \n",
       "\n",
       "[5 rows x 590 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming the columns \n",
    "secom_data.columns = [\"Feature_\"+str(column+1) for column in range(len(secom_data.columns))]\n",
    "secom_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1m82GHOT1msU"
   },
   "outputs": [],
   "source": [
    "# Importing target data from internet in runtime\n",
    "label_url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MXNuzPJe1sNk"
   },
   "outputs": [],
   "source": [
    "# Assigning target data to a pandas dataframe\n",
    "secom_labels = pd.read_csv(label_url, sep = \" \",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RqtL5guL1tz7"
   },
   "outputs": [],
   "source": [
    "# Renaming the columns \n",
    "secom_labels.columns = [\"Classification\",\"Timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "84Dl-Fay1vz3"
   },
   "outputs": [],
   "source": [
    "# Changing datatype for consitency\n",
    "secom_labels['Timestamp'] = pd.to_datetime(secom_labels['Timestamp'],errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qSmgspyk1xcL"
   },
   "outputs": [],
   "source": [
    "# Merging the data\n",
    "data= pd.concat([secom_labels,secom_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "foQ0Zplo1yxq"
   },
   "outputs": [],
   "source": [
    "# Dropping the timestamp column as it provides no insights\n",
    "data.drop(columns=\"Timestamp\", errors='raise',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wSd7j_0210WF"
   },
   "outputs": [],
   "source": [
    "target = data[['Classification']]\n",
    "data.drop(['Classification'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcnUqd02122m"
   },
   "source": [
    "#### Spliting the data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UfkKHvKQ14hm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,target, test_size=0.20, random_state=42, stratify=target)\n",
    "# We use the startify parameter as the data is imbalanced on the basis of pass and fail and this will ensure the same \n",
    "#is retained when we split - Random state will help us create a reproducible data - Test size is the split ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zoPcLGkRzCNR"
   },
   "outputs": [],
   "source": [
    "X_test_original = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VngQ8qmq18L2"
   },
   "source": [
    "#### Re-labeling the Target values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Zita67WD18oW"
   },
   "outputs": [],
   "source": [
    "# # Relabeling target variables 1 is pass and 0 is fail\n",
    "# y_train = y_train.replace(to_replace=[-1, 1], value=[1, 0])\n",
    "# y_test = y_test.replace(to_replace=[-1, 1], value=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Qqf5h04o2BV3"
   },
   "outputs": [],
   "source": [
    "# # Scaling the data\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "# X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ktw4-If-2JVo"
   },
   "source": [
    "# Decision : Removing columns with more that 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sealjvqy16tt",
    "outputId": "7671024d-d5e9-4c43-88f9-a3a1608a1dbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1253, 590)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3sSkLKz2MQv",
    "outputId": "8a6cef88-7ace-43d7-f84d-af30c1eb14d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Features deleted with more than 0.5 % missing values 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1253, 558)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_missing = X_train\n",
    "missing_remove(X_train_missing, 0.5)\n",
    "X_train_missing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMtATkmGbzmV",
    "outputId": "29775b18-3ca7-4f18-9f8a-78805b9ce04c"
   },
   "source": [
    "## Removing Unique cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_unique = X_train_missing\n",
    "# unique_remove(X_train_unique)\n",
    "# X_train_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 558)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From test set deleting columns that are removed in training set\n",
    "X_test_missing = X_test[np.array(X_train_missing.columns)]\n",
    "X_test_missing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGCsY1go4BPk"
   },
   "source": [
    "# Decision : Replacing outliers with 3s boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GF5OKR37z9k"
   },
   "source": [
    "##### Other option of removing outliers and replacing them has been explored in file \"Base file - removing outliers and replacing with knn.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8nKubTxv2Qio"
   },
   "outputs": [],
   "source": [
    "def replace_outlier(val, mean, std):\n",
    "    if val > mean + 3*std:\n",
    "        return mean + 3*std \n",
    "    elif val < mean - 3*std:\n",
    "        return mean - 3*std\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dR6rW4Sa1jux"
   },
   "outputs": [],
   "source": [
    "# replace with 3s in X_train\n",
    "for col in X_train_missing.columns:\n",
    "    mean = X_train_missing[col].mean()\n",
    "    std_dev = X_train_missing[col].std(axis=0)\n",
    "    X_train_missing[col] = X_train_missing[col].map(lambda x: replace_outlier(x, mean, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FSHmjvRz1lkY"
   },
   "outputs": [],
   "source": [
    "# replace with 3s in X_test\n",
    "for col in X_test_missing.columns:\n",
    "    mean = X_test_missing[col].mean()\n",
    "    std_dev = X_test_missing[col].std(axis=0)\n",
    "    X_test_missing[col] = X_test_missing[col].map(lambda x: replace_outlier(x, mean, std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ_qt_Kt1-Mj"
   },
   "source": [
    "# KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVUDFiOD2B3i",
    "outputId": "bad68faa-ed38-4968-fdbf-aa9afa8a0c1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn = KNNImputer()\n",
    "knn.fit(X_train_missing)\n",
    "X_train_knn=pd.DataFrame(knn.transform(X_train_missing), columns=X_train_missing.columns)\n",
    "X_train_knn.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O-i15vi2RNR",
    "outputId": "5e784803-080a-4af1-9776-24ee241b632c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "knn = KNNImputer()\n",
    "knn.fit(X_test_missing)\n",
    "X_test_knn=pd.DataFrame(knn.transform(X_test_missing), columns=X_test_missing.columns)\n",
    "X_test_knn.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyBswGn5Pq1V"
   },
   "source": [
    "# Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "knn - boruta.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
